{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOkz8HM/4NfWGLtQ/iw2S5W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Overfitting\n","\n","Overfitting occurs when a model learns too much from the training data, including noise, leading to poor generalization. Underfitting happens when the model is too simple to capture patterns."],"metadata":{"id":"Z6mYtcEGdMgS"}},{"cell_type":"markdown","source":["# Signs of Overfitting\n","1. High Training Accuracy, Low Test Accuracy – If your model performs exceptionally well on training data but poorly on test data, it may be overfitting.\n","\n","2. Large Gap Between Training and Validation Loss – A significant difference between training and validation loss indicates overfitting.\n","\n","3. Complex Model with Too Many Parameters – Overly complex models tend to memorize training data rather than generalizing.\n","\n","4. Performance Decreases on New Data – If the model struggles with unseen data, it may have learned patterns specific to the training set.\n","\n","##Methods to Detect Overfitting\n","1. Learning Curves – Plot training and validation accuracy/loss over epochs. If validation loss increases while training loss decreases, overfitting is likely.\n","\n","2. Cross-Validation – Using techniques like k-fold cross-validation helps assess generalization.\n","\n","3. Regularization Techniques – L1/L2 regularization, dropout, and early stopping can help mitigate overfitting."],"metadata":{"id":"Sj7kx6zLeMHT"}},{"cell_type":"markdown","source":["# Underfitting"],"metadata":{"id":"YqRax7HbeiOf"}},{"cell_type":"markdown","source":["# Signs of Underfitting\n","1. High Error Rates on Both Training and Validation Data – If the model performs poorly on both datasets, it may be underfitting.\n","\n","2. Low Model Complexity – Using a simple model (e.g., linear regression for non-linear data) can lead to underfitting.\n","\n","3. Failure to Capture Trends – Visualizing predictions vs. actual data can reveal if the model fails to learn patterns.\n","\n","4. Bias-Variance Tradeoff – High bias and low variance indicate underfitting, meaning the model is too rigid.\n","\n","# How to Address Underfitting\n","1. Increase Model Complexity – Use deeper neural networks or more features.\n","\n","2. Train for More Epochs – Allow the model to learn longer before stopping.\n","\n","3. Reduce Regularization – Excessive dropout or weight decay can prevent learning.\n","\n","4. Improve Feature Engineering – Ensure relevant features are included"],"metadata":{"id":"FNZBWImJekUE"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEErVcjidCPS","executionInfo":{"status":"ok","timestamp":1748512812337,"user_tz":-330,"elapsed":346996,"user":{"displayName":"Meenatchi K V","userId":"15684038064487154879"}},"outputId":"99dbc004-f85d-46b8-f148-f79fee38002f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 17.4MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 472kB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 4.35MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 7.57MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.2142\n","Epoch 2, Loss: 0.1167\n","Epoch 3, Loss: 0.2253\n","Epoch 4, Loss: 0.1279\n","Epoch 5, Loss: 0.0180\n","Epoch 6, Loss: 0.1655\n","Epoch 7, Loss: 0.1267\n","Epoch 8, Loss: 0.0335\n","Epoch 9, Loss: 0.0007\n","Epoch 10, Loss: 0.0104\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# Load dataset\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# Define a simple neural network\n","class OverfitNet(nn.Module):\n","    def __init__(self):\n","        super(OverfitNet, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 512)\n","        self.fc2 = nn.Linear(512, 512)\n","        self.fc3 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Initialize model, loss function, and optimizer\n","model = OverfitNet()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train the model (overfitting likely due to large capacity)\n","for epoch in range(10):\n","    for images, labels in train_loader:\n","        optimizer.zero_grad()\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        loss.backward()\n","        optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"]},{"cell_type":"markdown","source":["# Regularization (L2 Weight Decay)\n","\n","```\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 Regularization\n","```\n","\n"],"metadata":{"id":"LCTNvO0Re1ID"}},{"cell_type":"markdown","source":["# Dropout Regularization\n","\n","Dropout randomly disables neurons during training to prevent reliance on specific features."],"metadata":{"id":"Q4HNaL3tfIfM"}},{"cell_type":"code","source":["class DropoutNet(nn.Module):\n","    def __init__(self):\n","        super(DropoutNet, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 512)\n","        self.dropout = nn.Dropout(0.5)  # Dropout layer\n","        self.fc2 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.fc2(x)\n","        return x\n","\n","# Initialize model with dropout\n","model_dropout = DropoutNet()\n","optimizer_dropout = optim.Adam(model_dropout.parameters(), lr=0.001)\n"],"metadata":{"id":"7E2484SGdsDr","executionInfo":{"status":"ok","timestamp":1748512876829,"user_tz":-330,"elapsed":3,"user":{"displayName":"Meenatchi K V","userId":"15684038064487154879"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Early Stopping\n","\n","Early stopping halts training when validation loss stops improving"],"metadata":{"id":"kIkEnyuPfTUd"}},{"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=3):\n","        self.patience = patience\n","        self.best_loss = float(\"inf\")\n","        self.counter = 0\n","\n","    def check(self, val_loss):\n","        if val_loss < self.best_loss:\n","            self.best_loss = val_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                print(\"Early stopping triggered!\")\n","                return True\n","        return False\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"LVnJG8LyfSgW","executionInfo":{"status":"ok","timestamp":1748513032140,"user_tz":-330,"elapsed":10,"user":{"displayName":"Meenatchi K V","userId":"15684038064487154879"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_0Dp4OcJfeEm"},"execution_count":null,"outputs":[]}]}